<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  HCVL@UoB


  | Research

</title>
<meta name="description" content="Dr. Hyung Jin Chang, Assistant Professor
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>"> -->
<link rel="shortcut icon" type="image/x-icon" href="/assets/img/favicon.ico">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://cvlab-uob.github.io/research/">



  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://cvlab-uob.github.io/">
       <!-- HCVL@UoB -->
       <b>Human-Centred Visual Learning Group</b>
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <!-- <li class="nav-item "> -->
            <li class="nav-item active">
            <a class="nav-link" href="/">
              H.J.Chang
              
            </a>
          </li>
          
          <!-- Blog -->
          <!-- <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li> -->
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/research/">
                Research
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/team/">
                Team
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/service/">
                Openings
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item dropdown ">
              <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                Others
                
              </a>
              <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown">
              
              
                <a class="dropdown-item" href="/teaching/">Teaching</a>
              
              
              
                <div class="dropdown-divider"></div>
              
              
              
                <a class="dropdown-item" href="/contact/">Contact</a>
              
              
              </div>
          </li>
          
          
          
          
          
          
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title"><strong>Research</strong></h1>
    <p class="post-description"></p>
  </header>

  <article>
    <hr>
<h2 id="eye-gaze-tracking"><strong>Eye Gaze Tracking</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/research/gaze.gif-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/research/gaze.gif-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/research/gaze.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/research/gaze.gif" title="">

  </picture>

  

</figure>

    </div>
</div>

<p><strong>Eye gaze</strong> is an important functional component in various applications, as it
indicates human attentiveness and can thus be used to study their intentions
and understand social interactions. For these reasons, accurately estimating
gaze is an active research topic in computer vision, with applications in affect
analysis, saliency detection and action recognition, to name
a few. Gaze estimation has also been applied in domains other than computer
vision, such as navigation for eye gaze controlled wheelchairs, detection
of non-verbal behaviors of drivers, and inferring the object of interest in
human-robot interactions.</p>

<p>Research Output: [<a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.pdf" target="_blank" rel="noopener noreferrer">RT-GENE(ECCV2018)</a>], [<a href="">CVPRW’22</a>]</p>

<p>Researchers: Hengfei and Nora</p>

<hr>
<h2 id="hand-and-object-interaction-modeling"><strong>Hand and Object Interaction Modeling</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/research/HO-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/research/HO-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/research/HO-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/research/HO.jpg" title="">

  </picture>

  

</figure>

    </div>
</div>
<!-- <div class="caption">
    Example Submodular Settings
</div> -->

<p><strong>Estimating the pose and shape of hands and objects</strong> under interaction finds numerous applications including augmented and virtual reality. <strong>Existing approaches for hand and object reconstruction</strong> require explicitly defined physical
constraints and known objects, which limits its application
domains. Our algorithm is agnostic to object models, and it
learns the physical rules governing hand-object interaction.
This requires automatically inferring the shapes and physical interaction of hands and (potentially unknown) objects.
We seek to approach this challenging problem by proposing a collaborative learning strategy where two-branches
of deep networks are learning from each other. Specifically,
we transfer hand mesh information to the object branch
and vice versa for the hand branch. The resulting optimisation (training) problem can be unstable, and we address
this via two strategies: (i) attention-guided graph convolution which helps identify and focus on mutual occlusion
and (ii) unsupervised associative loss which facilitates the
transfer of information between the branches. Experiments
using four widely-used benchmarks show that our framework achieves beyond state-of-the-art accuracy in 3D pose
estimation, as well as recovers dense 3D hand and object
shapes. Each technical component above contributes meaningfully in the ablation study.</p>

<p>Research Output: [<a href="https://arxiv.org/pdf/2204.13062.pdf" target="_blank" rel="noopener noreferrer">CVPR’22</a>], [<a href="https://arxiv.org/abs/2007.05168" target="_blank" rel="noopener noreferrer">SeqHAND(ECCV’22)</a>], [<a href="http://www.iis.ee.ic.ac.uk/~dtang/cvpr_14.pdf" target="_blank" rel="noopener noreferrer">CVPR’14</a>], [<a href="https://hyungjinchang.files.wordpress.com/2015/03/latentregressionforest_tpami.pdf" target="_blank" rel="noopener noreferrer">TPAMI’16</a>]</p>

<p>Researchers: Elden and Zhongqun</p>

<hr>
<h2 id="6d-object-pose-estimation"><strong>6D Object Pose Estimation</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/research/6D-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/research/6D-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/research/6D-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/research/6D.jpg" title="">

  </picture>

  

</figure>

    </div>
</div>

<p><strong>Fast and accurate tracking of an object’s motion</strong>
is one of the key functionalities of a robotic system for achieving
reliable interaction with the environment. This paper focuses on
the instance-level six-dimensional (6D) pose tracking problem
with a symmetric and textureless object under occlusion. We
propose a Temporally Primed 6D pose tracking framework with
Auto-Encoders (TP-AE) to tackle the pose tracking problem.
The framework consists of a prediction step and a temporally
primed pose estimation step. The prediction step aims to quickly
and efficiently generate a guess on the object’s real-time pose
based on historical information about the target object’s motion.
Once the prior prediction is obtained, the temporally primed
pose estimation step embeds the prior pose into the RGB-D
input, and leverages auto-encoders to reconstruct the target
object with higher quality under occlusion, thus improving the
framework’s performance. Extensive experiments show that the
proposed 6D pose tracking method can accurately estimate the
6D pose of a symmetric and textureless object under occlusion,
and significantly outperforms the state-of-the-art on T-LESS
dataset while running in real-time at 26 FPS.</p>

<p>Research Output: [<a href="http://pure-oai.bham.ac.uk/ws/portalfiles/portal/164770788/_ICRA_TP_AE_6D_Object_Tracking.pdf" target="_blank" rel="noopener noreferrer">TP-AE(ICRA’22)</a>], [<a href="http://openaccess.thecvf.com/content/CVPR2021/papers/Chen_FS-Net_Fast_Shape-Based_Network_for_Category-Level_6D_Object_Pose_Estimation_CVPR_2021_paper.pdf" target="_blank" rel="noopener noreferrer">FS-Net(CVPR’21 Oral)</a>], [<a href="http://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_G2L-Net_Global_to_Local_Network_for_Real-Time_6D_Pose_Estimation_CVPR_2020_paper.pdf" target="_blank" rel="noopener noreferrer">G2L-Net(CVPR’20)</a>], [<a href="http://openaccess.thecvf.com/content_WACV_2020/papers/Chen_PonitPoseNet_Point_Pose_Network_for_Robust_6D_Object_Pose_Estimation_WACV_2020_paper.pdf" target="_blank" rel="noopener noreferrer">PPNet(WACV’20)</a>]</p>

<p>Researchers: Linfang and Wei</p>

<hr>
<h2 id="neural-radiance-field-nerf"><strong>Neural Radiance Field (NeRF)</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/research/NeRF-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/research/NeRF-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/research/NeRF-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/research/NeRF.jpg" title="">

  </picture>

  

</figure>

    </div>
</div>

<p>Imagine you have a photo of yourself where everything
is perfect except the camera angle. Perhaps the photographer cut off the top of the attraction you were standing in
front of, or you wished the photo had a wider field-of-view.
The best course of action to correct this would be to retake the photo immediately after looking at the outcomes,
but this is not always possible, especially for touristic locations, which are often the photos that you care about a lot.
Moreover, in situations where there is no “second try”, for
example when your baby walks for the first time, a re-take
is not an option.</p>

<p>Research Output: [<a href="https://openaccess.thecvf.com/content/WACV2022/papers/Freer_Novel-View_Synthesis_of_Human_Tourist_Photos_WACV_2022_paper.pdf" target="_blank" rel="noopener noreferrer">WACV’22</a>]</p>

<p>Researchers: Jonathan and Hengfei</p>

<h2 id="visual-object-tracking"><strong>Visual Object Tracking</strong></h2>

<div class="row justify-content-sm-center">
    <div class="col-sm mt-3 mt-md-0">
        

<figure>

  <picture>
    
      <source media="(max-width: 480px)" srcset="/assets/img/research/tracking-480.webp"></source>
    
      <source media="(max-width: 800px)" srcset="/assets/img/research/tracking-800.webp"></source>
    
      <source media="(max-width: 1400px)" srcset="/assets/img/research/tracking-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img class="img-fluid rounded z-depth-1" src="/assets/img/research/tracking.jpg" title="">

  </picture>

  

</figure>

    </div>
</div>

<p>Key to realizing the vision of human-centred computing is the ability for machines to recognize people, so that spaces and devices can become truly personalized. However, the unpredictability of real-world environments impacts robust recognition, limiting usability. In real conditions, human identification systems have to handle issues such as out-of-set subjects and domain deviations, where conventional supervised learning approaches for training and inference are poorly suited. With the rapid development of Internet of Things (IoT), we advocate a new labelling method that exploits signals of opportunity hidden in heterogeneous IoT data. The key insight is that <strong>one sensor modality can leverage the signals measured by other co-located sensor modalities to improve its own labelling performance</strong>. If identity associations between heterogeneous sensor data can be discovered, it is possible to automatically label data, leading to more robust human recognition, without manual labelling or enrolment. On the other side of the coin, we also study the privacy implication for such cross-modal identity association.</p>

<p>Research Output: [<a href="https://hyungjinchang.files.wordpress.com/2018/03/cvpr2018-traca-preprint.pdf" target="_blank" rel="noopener noreferrer">CVPR’18</a>], [<a href="https://hyungjinchang.files.wordpress.com/2015/03/attentional-correlation-filter-network-for-adaptive-visual-tracking.pdf" target="_blank" rel="noopener noreferrer">CVPR’17</a>], [<a href="https://hyungjinchang.files.wordpress.com/2015/03/cvpr2016-visualtracking.pdf" target="_blank" rel="noopener noreferrer">CVPR’16</a>], [<a href="">CVPR’13</a>]</p>

<p>Researches: Jinyu and Zhongqun</p>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    <!-- &copy; Copyright 2023 Human-Centred Visual Learning Group Dr. Hyung Jin Chang . -->
    © Copyright 2023 Dr. Hyung Jin Chang .
    
    
    
    Last updated: August 28, 2023.
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  





</html>
